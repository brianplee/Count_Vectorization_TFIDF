{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-Vectorizing words and calculating TF-IDF Scores from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/BPLEE/Desktop/Python/News Articles.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A second attempt to pass a bipartisan disaster...</td>\n",
       "      <td>Disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sweeping tax law Republicans enacted in la...</td>\n",
       "      <td>Economic Policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senate Majority Leader Mitch McConnell (R-Ky.)...</td>\n",
       "      <td>Court</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article            Topic\n",
       "0  A second attempt to pass a bipartisan disaster...         Disaster\n",
       "1  The sweeping tax law Republicans enacted in la...  Economic Policy\n",
       "2  Senate Majority Leader Mitch McConnell (R-Ky.)...            Court"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A second attempt to pass a bipartisan disaster aid package has failed after a House Republican voted on Tuesday against a bill that would secure $19 billion in emergency relief.  Rep. Thomas Massie (R-Ky.) objected to the legislation’s passage during a voice vote and asked that another vote be held after the House returns from recess next week.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A second attempt to pass a bipartisan disaster aid package has failed after a House Republican voted on Tuesday against a bill that would secure $19 billion in emergency relief.  Rep. Thomas Massie (R-Ky.) objected to the legislation’s passage during a voice vote and asked that another vote be held after the House returns from recess next week.',\n",
       " 'The sweeping tax law Republicans enacted in late 2017 is definitely not paying for itself and has not significantly boosted the economy or increased wages, the non-partisan Congressional Research Service said in a report.  But in line with what critics cautioned, the measure triggered a wave of corporate stock buybacks that benefited investors more than anybody else, according to the new study.',\n",
       " 'Senate Majority Leader Mitch McConnell (R-Ky.) said Tuesday he would work to fill any Supreme Court vacancy in 2020, an election year, despite his efforts to scuttle Judge Merrick Garland’s nomination to the bench for that very reason in 2016.  “Uh, we’d fill it,” McConnell said in response to an audience question during an event at the Paducah Chamber of Commerce in Kentucky on Tuesday afternoon. The lawmaker issued a small smile during his answer as guests in the room laughed.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store documents in a list via iteritems() in order to clean/process iteratively \n",
    "docs = []\n",
    "for i, article in df['Article'].iteritems():\n",
    "    docs.append(article)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process documents and tokenize individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes in a list of documents, removes special chars, white spaces, stop words, \n",
    "# tokenizes words, and adds them to a new list. \n",
    "\n",
    "tokenized_docs = []\n",
    "def clean_up(docs):\n",
    "    for doc in docs:\n",
    "        doc = doc.lower()\n",
    "        # Replace special characters with space\n",
    "        doc = re.sub('[^\\w\\s]', '', doc)\n",
    "        doc = re.sub('_', '', doc)\n",
    "        # Replace any whitespace with 1 space\n",
    "        doc = re.sub('\\s+', ' ', doc)\n",
    "        # Remove start and end whitespaces\n",
    "        doc = doc.strip()\n",
    "        # Tokenize each word by splitting by spaces\n",
    "        doc = doc.split(' ')\n",
    "        \n",
    "        # Remove stop words if desired (a, the, of, to, etc.)\n",
    "        \n",
    "        tokenized_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'second', 'attempt', 'to', 'pass', 'a', 'bipartisan', 'disaster', 'aid', 'package', 'has', 'failed', 'after', 'a', 'house', 'republican', 'voted', 'on', 'tuesday', 'against', 'a', 'bill', 'that', 'would', 'secure', '19', 'billion', 'in', 'emergency', 'relief', 'rep', 'thomas', 'massie', 'rky', 'objected', 'to', 'the', 'legislations', 'passage', 'during', 'a', 'voice', 'vote', 'and', 'asked', 'that', 'another', 'vote', 'be', 'held', 'after', 'the', 'house', 'returns', 'from', 'recess', 'next', 'week'], ['the', 'sweeping', 'tax', 'law', 'republicans', 'enacted', 'in', 'late', '2017', 'is', 'definitely', 'not', 'paying', 'for', 'itself', 'and', 'has', 'not', 'significantly', 'boosted', 'the', 'economy', 'or', 'increased', 'wages', 'the', 'nonpartisan', 'congressional', 'research', 'service', 'said', 'in', 'a', 'report', 'but', 'in', 'line', 'with', 'what', 'critics', 'cautioned', 'the', 'measure', 'triggered', 'a', 'wave', 'of', 'corporate', 'stock', 'buybacks', 'that', 'benefited', 'investors', 'more', 'than', 'anybody', 'else', 'according', 'to', 'the', 'new', 'study'], ['senate', 'majority', 'leader', 'mitch', 'mcconnell', 'rky', 'said', 'tuesday', 'he', 'would', 'work', 'to', 'fill', 'any', 'supreme', 'court', 'vacancy', 'in', '2020', 'an', 'election', 'year', 'despite', 'his', 'efforts', 'to', 'scuttle', 'judge', 'merrick', 'garlands', 'nomination', 'to', 'the', 'bench', 'for', 'that', 'very', 'reason', 'in', '2016', 'uh', 'wed', 'fill', 'it', 'mcconnell', 'said', 'in', 'response', 'to', 'an', 'audience', 'question', 'during', 'an', 'event', 'at', 'the', 'paducah', 'chamber', 'of', 'commerce', 'in', 'kentucky', 'on', 'tuesday', 'afternoon', 'the', 'lawmaker', 'issued', 'a', 'small', 'smile', 'during', 'his', 'answer', 'as', 'guests', 'in', 'the', 'room', 'laughed']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a corpus of all words with index positions in a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a corpus containing all words across all documents\n",
    "# Assign an index position to each \n",
    "corpus = {}\n",
    "i = 1\n",
    "for doc in tokenized_docs:\n",
    "    for word in doc:\n",
    "        if word in corpus:\n",
    "            continue\n",
    "        corpus[word] = i\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'second': 2, 'attempt': 3, 'to': 4, 'pass': 5, 'bipartisan': 6, 'disaster': 7, 'aid': 8, 'package': 9, 'has': 10, 'failed': 11, 'after': 12, 'house': 13, 'republican': 14, 'voted': 15, 'on': 16, 'tuesday': 17, 'against': 18, 'bill': 19, 'that': 20, 'would': 21, 'secure': 22, '19': 23, 'billion': 24, 'in': 25, 'emergency': 26, 'relief': 27, 'rep': 28, 'thomas': 29, 'massie': 30, 'rky': 31, 'objected': 32, 'the': 33, 'legislations': 34, 'passage': 35, 'during': 36, 'voice': 37, 'vote': 38, 'and': 39, 'asked': 40, 'another': 41, 'be': 42, 'held': 43, 'returns': 44, 'from': 45, 'recess': 46, 'next': 47, 'week': 48, 'sweeping': 49, 'tax': 50, 'law': 51, 'republicans': 52, 'enacted': 53, 'late': 54, '2017': 55, 'is': 56, 'definitely': 57, 'not': 58, 'paying': 59, 'for': 60, 'itself': 61, 'significantly': 62, 'boosted': 63, 'economy': 64, 'or': 65, 'increased': 66, 'wages': 67, 'nonpartisan': 68, 'congressional': 69, 'research': 70, 'service': 71, 'said': 72, 'report': 73, 'but': 74, 'line': 75, 'with': 76, 'what': 77, 'critics': 78, 'cautioned': 79, 'measure': 80, 'triggered': 81, 'wave': 82, 'of': 83, 'corporate': 84, 'stock': 85, 'buybacks': 86, 'benefited': 87, 'investors': 88, 'more': 89, 'than': 90, 'anybody': 91, 'else': 92, 'according': 93, 'new': 94, 'study': 95, 'senate': 96, 'majority': 97, 'leader': 98, 'mitch': 99, 'mcconnell': 100, 'he': 101, 'work': 102, 'fill': 103, 'any': 104, 'supreme': 105, 'court': 106, 'vacancy': 107, '2020': 108, 'an': 109, 'election': 110, 'year': 111, 'despite': 112, 'his': 113, 'efforts': 114, 'scuttle': 115, 'judge': 116, 'merrick': 117, 'garlands': 118, 'nomination': 119, 'bench': 120, 'very': 121, 'reason': 122, '2016': 123, 'uh': 124, 'wed': 125, 'it': 126, 'response': 127, 'audience': 128, 'question': 129, 'event': 130, 'at': 131, 'paducah': 132, 'chamber': 133, 'commerce': 134, 'kentucky': 135, 'afternoon': 136, 'lawmaker': 137, 'issued': 138, 'small': 139, 'smile': 140, 'answer': 141, 'as': 142, 'guests': 143, 'room': 144, 'laughed': 145}\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize each doc with n=len(corpus) dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors of zero of each document\n",
    "# Iterate through each word in each doc, and add 1 to the word's index position \n",
    "\n",
    "# Note: vectors are (n x 1) matrices, but are rewritten horizontally with commas btwn dimensions\n",
    "\n",
    "vectorized_docs = []\n",
    "def vectorize(tokenized_docs):\n",
    "    i = 1\n",
    "    for doc in tokenized_docs:\n",
    "        zeros_vector = [f'Doc{i}'] + [0]*len(corpus)\n",
    "        vectorized_docs.append(zeros_vector)\n",
    "        for word in doc:\n",
    "            zeros_vector[corpus[word]] = zeros_vector[corpus[word]]+1\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Doc1', 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['Doc2', 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['Doc3', 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_docs[0])\n",
    "print(vectorized_docs[1])\n",
    "print(vectorized_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Term Frequency TF(t) scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF(t) = (# of times term t appears in doc) / (total number of terms in doc)\n",
    "# IDF(t,d) = log(Total number of docs / number of docs with term t in it)  math.log(doc#/doc(t))\n",
    "# Tf-idf = TF x IDF\n",
    "\n",
    "# TF scores\n",
    "tf_scores = []\n",
    "def tf_score(vectorized_docs):\n",
    "    for doc in vectorized_docs:\n",
    "        tf_vectors = []\n",
    "        n = 0\n",
    "        for dimension in doc[1:]:\n",
    "            tf = dimension / sum(doc[1:])\n",
    "            tf_vectors.append((doc[0], f'{list(corpus)[n]}',tf))   # list(corpus)[0] = 'a'\n",
    "            n = n + 1\n",
    "        tf_scores.append(tf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_score(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Doc1', 'a', 0.08620689655172414), ('Doc1', 'second', 0.017241379310344827), ('Doc1', 'attempt', 0.017241379310344827), ('Doc1', 'to', 0.034482758620689655), ('Doc1', 'pass', 0.017241379310344827)]\n",
      "[('Doc2', 'a', 0.03225806451612903), ('Doc2', 'second', 0.0), ('Doc2', 'attempt', 0.0), ('Doc2', 'to', 0.016129032258064516), ('Doc2', 'pass', 0.0)]\n",
      "[('Doc3', 'a', 0.012345679012345678), ('Doc3', 'second', 0.0), ('Doc3', 'attempt', 0.0), ('Doc3', 'to', 0.04938271604938271), ('Doc3', 'pass', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(tf_scores[0][0:5])\n",
    "print(tf_scores[1][0:5])\n",
    "print(tf_scores[2][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate IDF(t, d) scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'second', 'attempt', 'to', 'pass']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview list(corpus)\n",
    "\n",
    "list(corpus)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF(t, d) scores - this is the weight of each word across all documents. \n",
    "# IDF(t, d) = log(Total number of docs / number of docs with term t in it)  math.log(doc#/doc(t))\n",
    "\n",
    "idf_scores = []\n",
    "docs_with_word = [0]*len(corpus)  # Vector of 146 zeros\n",
    "def idf_score(corpus, tokenized_docs):\n",
    "    i = 0 \n",
    "    for word in list(corpus):\n",
    "        for doc in tokenized_docs:\n",
    "            if word in doc:\n",
    "                docs_with_word[i] = docs_with_word[i] + 1\n",
    "        i = i + 1\n",
    "    print(f'No. of docs containing each word: {docs_with_word}')\n",
    "    \n",
    "    i = 0\n",
    "    for doc_num in docs_with_word:\n",
    "        idf = math.log(len(tokenized_docs) / doc_num)\n",
    "        idf_scores.append((list(corpus)[i], idf))\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of docs containing each word: [3, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 3, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "idf_score(corpus, tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0.0),\n",
       " ('second', 1.0986122886681098),\n",
       " ('attempt', 1.0986122886681098),\n",
       " ('to', 0.0),\n",
       " ('pass', 1.0986122886681098),\n",
       " ('bipartisan', 1.0986122886681098),\n",
       " ('disaster', 1.0986122886681098),\n",
       " ('aid', 1.0986122886681098),\n",
       " ('package', 1.0986122886681098),\n",
       " ('has', 0.4054651081081644)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'a' has a low score because it's present in all docs, therefore less meaningful \n",
    "\n",
    "idf_scores[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TF-IDF(t, d) scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Doc1', 'a', 0.08620689655172414), ('Doc1', 'second', 0.017241379310344827), ('Doc1', 'attempt', 0.017241379310344827), ('Doc1', 'to', 0.034482758620689655), ('Doc1', 'pass', 0.017241379310344827)]\n",
      "\n",
      "\n",
      "0.08620689655172414\n"
     ]
    }
   ],
   "source": [
    "# Preview tf_scores (list of lists)\n",
    "\n",
    "print(tf_scores[0][0:5])\n",
    "\n",
    "print('\\n')\n",
    "print(tf_scores[0][0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF score\n",
    "# Multiple the term frequency (TF) by the weight (IDF), for words in every doc\n",
    "\n",
    "tfidf_scores = []\n",
    "def tfidf_score(tf_scores, idf_scores):\n",
    "    for doc in tf_scores:\n",
    "        i = 0\n",
    "        doc_tfidf = []\n",
    "        for coord in doc: \n",
    "            tfidf = coord[2] * idf_scores[i][1]\n",
    "            doc_tfidf.append((coord[0], coord[1], idf_scores[i][0], tfidf))\n",
    "            i = i + 1\n",
    "        tfidf_scores.append(doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_score(tf_scores, idf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Doc1', 'a', 'a', 0.0),\n",
       "  ('Doc1', 'second', 'second', 0.01894159118393293),\n",
       "  ('Doc1', 'attempt', 'attempt', 0.01894159118393293),\n",
       "  ('Doc1', 'to', 'to', 0.0),\n",
       "  ('Doc1', 'pass', 'pass', 0.01894159118393293),\n",
       "  ('Doc1', 'bipartisan', 'bipartisan', 0.01894159118393293),\n",
       "  ('Doc1', 'disaster', 'disaster', 0.01894159118393293),\n",
       "  ('Doc1', 'aid', 'aid', 0.01894159118393293),\n",
       "  ('Doc1', 'package', 'package', 0.01894159118393293),\n",
       "  ('Doc1', 'has', 'has', 0.006990777726002834),\n",
       "  ('Doc1', 'failed', 'failed', 0.01894159118393293),\n",
       "  ('Doc1', 'after', 'after', 0.03788318236786586),\n",
       "  ('Doc1', 'house', 'house', 0.03788318236786586),\n",
       "  ('Doc1', 'republican', 'republican', 0.01894159118393293),\n",
       "  ('Doc1', 'voted', 'voted', 0.01894159118393293),\n",
       "  ('Doc1', 'on', 'on', 0.006990777726002834),\n",
       "  ('Doc1', 'tuesday', 'tuesday', 0.006990777726002834),\n",
       "  ('Doc1', 'against', 'against', 0.01894159118393293),\n",
       "  ('Doc1', 'bill', 'bill', 0.01894159118393293),\n",
       "  ('Doc1', 'that', 'that', 0.0),\n",
       "  ('Doc1', 'would', 'would', 0.006990777726002834),\n",
       "  ('Doc1', 'secure', 'secure', 0.01894159118393293),\n",
       "  ('Doc1', '19', '19', 0.01894159118393293),\n",
       "  ('Doc1', 'billion', 'billion', 0.01894159118393293),\n",
       "  ('Doc1', 'in', 'in', 0.0),\n",
       "  ('Doc1', 'emergency', 'emergency', 0.01894159118393293),\n",
       "  ('Doc1', 'relief', 'relief', 0.01894159118393293),\n",
       "  ('Doc1', 'rep', 'rep', 0.01894159118393293),\n",
       "  ('Doc1', 'thomas', 'thomas', 0.01894159118393293),\n",
       "  ('Doc1', 'massie', 'massie', 0.01894159118393293),\n",
       "  ('Doc1', 'rky', 'rky', 0.006990777726002834),\n",
       "  ('Doc1', 'objected', 'objected', 0.01894159118393293),\n",
       "  ('Doc1', 'the', 'the', 0.0),\n",
       "  ('Doc1', 'legislations', 'legislations', 0.01894159118393293),\n",
       "  ('Doc1', 'passage', 'passage', 0.01894159118393293),\n",
       "  ('Doc1', 'during', 'during', 0.006990777726002834),\n",
       "  ('Doc1', 'voice', 'voice', 0.01894159118393293),\n",
       "  ('Doc1', 'vote', 'vote', 0.03788318236786586),\n",
       "  ('Doc1', 'and', 'and', 0.006990777726002834),\n",
       "  ('Doc1', 'asked', 'asked', 0.01894159118393293),\n",
       "  ('Doc1', 'another', 'another', 0.01894159118393293),\n",
       "  ('Doc1', 'be', 'be', 0.01894159118393293),\n",
       "  ('Doc1', 'held', 'held', 0.01894159118393293),\n",
       "  ('Doc1', 'returns', 'returns', 0.01894159118393293),\n",
       "  ('Doc1', 'from', 'from', 0.01894159118393293),\n",
       "  ('Doc1', 'recess', 'recess', 0.01894159118393293),\n",
       "  ('Doc1', 'next', 'next', 0.01894159118393293),\n",
       "  ('Doc1', 'week', 'week', 0.01894159118393293),\n",
       "  ('Doc1', 'sweeping', 'sweeping', 0.0),\n",
       "  ('Doc1', 'tax', 'tax', 0.0),\n",
       "  ('Doc1', 'law', 'law', 0.0),\n",
       "  ('Doc1', 'republicans', 'republicans', 0.0),\n",
       "  ('Doc1', 'enacted', 'enacted', 0.0),\n",
       "  ('Doc1', 'late', 'late', 0.0),\n",
       "  ('Doc1', '2017', '2017', 0.0),\n",
       "  ('Doc1', 'is', 'is', 0.0),\n",
       "  ('Doc1', 'definitely', 'definitely', 0.0),\n",
       "  ('Doc1', 'not', 'not', 0.0),\n",
       "  ('Doc1', 'paying', 'paying', 0.0),\n",
       "  ('Doc1', 'for', 'for', 0.0),\n",
       "  ('Doc1', 'itself', 'itself', 0.0),\n",
       "  ('Doc1', 'significantly', 'significantly', 0.0),\n",
       "  ('Doc1', 'boosted', 'boosted', 0.0),\n",
       "  ('Doc1', 'economy', 'economy', 0.0),\n",
       "  ('Doc1', 'or', 'or', 0.0),\n",
       "  ('Doc1', 'increased', 'increased', 0.0),\n",
       "  ('Doc1', 'wages', 'wages', 0.0),\n",
       "  ('Doc1', 'nonpartisan', 'nonpartisan', 0.0),\n",
       "  ('Doc1', 'congressional', 'congressional', 0.0),\n",
       "  ('Doc1', 'research', 'research', 0.0),\n",
       "  ('Doc1', 'service', 'service', 0.0),\n",
       "  ('Doc1', 'said', 'said', 0.0),\n",
       "  ('Doc1', 'report', 'report', 0.0),\n",
       "  ('Doc1', 'but', 'but', 0.0),\n",
       "  ('Doc1', 'line', 'line', 0.0),\n",
       "  ('Doc1', 'with', 'with', 0.0),\n",
       "  ('Doc1', 'what', 'what', 0.0),\n",
       "  ('Doc1', 'critics', 'critics', 0.0),\n",
       "  ('Doc1', 'cautioned', 'cautioned', 0.0),\n",
       "  ('Doc1', 'measure', 'measure', 0.0),\n",
       "  ('Doc1', 'triggered', 'triggered', 0.0),\n",
       "  ('Doc1', 'wave', 'wave', 0.0),\n",
       "  ('Doc1', 'of', 'of', 0.0),\n",
       "  ('Doc1', 'corporate', 'corporate', 0.0),\n",
       "  ('Doc1', 'stock', 'stock', 0.0),\n",
       "  ('Doc1', 'buybacks', 'buybacks', 0.0),\n",
       "  ('Doc1', 'benefited', 'benefited', 0.0),\n",
       "  ('Doc1', 'investors', 'investors', 0.0),\n",
       "  ('Doc1', 'more', 'more', 0.0),\n",
       "  ('Doc1', 'than', 'than', 0.0),\n",
       "  ('Doc1', 'anybody', 'anybody', 0.0),\n",
       "  ('Doc1', 'else', 'else', 0.0),\n",
       "  ('Doc1', 'according', 'according', 0.0),\n",
       "  ('Doc1', 'new', 'new', 0.0),\n",
       "  ('Doc1', 'study', 'study', 0.0),\n",
       "  ('Doc1', 'senate', 'senate', 0.0),\n",
       "  ('Doc1', 'majority', 'majority', 0.0),\n",
       "  ('Doc1', 'leader', 'leader', 0.0),\n",
       "  ('Doc1', 'mitch', 'mitch', 0.0),\n",
       "  ('Doc1', 'mcconnell', 'mcconnell', 0.0),\n",
       "  ('Doc1', 'he', 'he', 0.0),\n",
       "  ('Doc1', 'work', 'work', 0.0),\n",
       "  ('Doc1', 'fill', 'fill', 0.0),\n",
       "  ('Doc1', 'any', 'any', 0.0),\n",
       "  ('Doc1', 'supreme', 'supreme', 0.0),\n",
       "  ('Doc1', 'court', 'court', 0.0),\n",
       "  ('Doc1', 'vacancy', 'vacancy', 0.0),\n",
       "  ('Doc1', '2020', '2020', 0.0),\n",
       "  ('Doc1', 'an', 'an', 0.0),\n",
       "  ('Doc1', 'election', 'election', 0.0),\n",
       "  ('Doc1', 'year', 'year', 0.0),\n",
       "  ('Doc1', 'despite', 'despite', 0.0),\n",
       "  ('Doc1', 'his', 'his', 0.0),\n",
       "  ('Doc1', 'efforts', 'efforts', 0.0),\n",
       "  ('Doc1', 'scuttle', 'scuttle', 0.0),\n",
       "  ('Doc1', 'judge', 'judge', 0.0),\n",
       "  ('Doc1', 'merrick', 'merrick', 0.0),\n",
       "  ('Doc1', 'garlands', 'garlands', 0.0),\n",
       "  ('Doc1', 'nomination', 'nomination', 0.0),\n",
       "  ('Doc1', 'bench', 'bench', 0.0),\n",
       "  ('Doc1', 'very', 'very', 0.0),\n",
       "  ('Doc1', 'reason', 'reason', 0.0),\n",
       "  ('Doc1', '2016', '2016', 0.0),\n",
       "  ('Doc1', 'uh', 'uh', 0.0),\n",
       "  ('Doc1', 'wed', 'wed', 0.0),\n",
       "  ('Doc1', 'it', 'it', 0.0),\n",
       "  ('Doc1', 'response', 'response', 0.0),\n",
       "  ('Doc1', 'audience', 'audience', 0.0),\n",
       "  ('Doc1', 'question', 'question', 0.0),\n",
       "  ('Doc1', 'event', 'event', 0.0),\n",
       "  ('Doc1', 'at', 'at', 0.0),\n",
       "  ('Doc1', 'paducah', 'paducah', 0.0),\n",
       "  ('Doc1', 'chamber', 'chamber', 0.0),\n",
       "  ('Doc1', 'commerce', 'commerce', 0.0),\n",
       "  ('Doc1', 'kentucky', 'kentucky', 0.0),\n",
       "  ('Doc1', 'afternoon', 'afternoon', 0.0),\n",
       "  ('Doc1', 'lawmaker', 'lawmaker', 0.0),\n",
       "  ('Doc1', 'issued', 'issued', 0.0),\n",
       "  ('Doc1', 'small', 'small', 0.0),\n",
       "  ('Doc1', 'smile', 'smile', 0.0),\n",
       "  ('Doc1', 'answer', 'answer', 0.0),\n",
       "  ('Doc1', 'as', 'as', 0.0),\n",
       "  ('Doc1', 'guests', 'guests', 0.0),\n",
       "  ('Doc1', 'room', 'room', 0.0),\n",
       "  ('Doc1', 'laughed', 'laughed', 0.0)],\n",
       " [('Doc2', 'a', 'a', 0.0),\n",
       "  ('Doc2', 'second', 'second', 0.0),\n",
       "  ('Doc2', 'attempt', 'attempt', 0.0),\n",
       "  ('Doc2', 'to', 'to', 0.0),\n",
       "  ('Doc2', 'pass', 'pass', 0.0),\n",
       "  ('Doc2', 'bipartisan', 'bipartisan', 0.0),\n",
       "  ('Doc2', 'disaster', 'disaster', 0.0),\n",
       "  ('Doc2', 'aid', 'aid', 0.0),\n",
       "  ('Doc2', 'package', 'package', 0.0),\n",
       "  ('Doc2', 'has', 'has', 0.006539759808196199),\n",
       "  ('Doc2', 'failed', 'failed', 0.0),\n",
       "  ('Doc2', 'after', 'after', 0.0),\n",
       "  ('Doc2', 'house', 'house', 0.0),\n",
       "  ('Doc2', 'republican', 'republican', 0.0),\n",
       "  ('Doc2', 'voted', 'voted', 0.0),\n",
       "  ('Doc2', 'on', 'on', 0.0),\n",
       "  ('Doc2', 'tuesday', 'tuesday', 0.0),\n",
       "  ('Doc2', 'against', 'against', 0.0),\n",
       "  ('Doc2', 'bill', 'bill', 0.0),\n",
       "  ('Doc2', 'that', 'that', 0.0),\n",
       "  ('Doc2', 'would', 'would', 0.0),\n",
       "  ('Doc2', 'secure', 'secure', 0.0),\n",
       "  ('Doc2', '19', '19', 0.0),\n",
       "  ('Doc2', 'billion', 'billion', 0.0),\n",
       "  ('Doc2', 'in', 'in', 0.0),\n",
       "  ('Doc2', 'emergency', 'emergency', 0.0),\n",
       "  ('Doc2', 'relief', 'relief', 0.0),\n",
       "  ('Doc2', 'rep', 'rep', 0.0),\n",
       "  ('Doc2', 'thomas', 'thomas', 0.0),\n",
       "  ('Doc2', 'massie', 'massie', 0.0),\n",
       "  ('Doc2', 'rky', 'rky', 0.0),\n",
       "  ('Doc2', 'objected', 'objected', 0.0),\n",
       "  ('Doc2', 'the', 'the', 0.0),\n",
       "  ('Doc2', 'legislations', 'legislations', 0.0),\n",
       "  ('Doc2', 'passage', 'passage', 0.0),\n",
       "  ('Doc2', 'during', 'during', 0.0),\n",
       "  ('Doc2', 'voice', 'voice', 0.0),\n",
       "  ('Doc2', 'vote', 'vote', 0.0),\n",
       "  ('Doc2', 'and', 'and', 0.006539759808196199),\n",
       "  ('Doc2', 'asked', 'asked', 0.0),\n",
       "  ('Doc2', 'another', 'another', 0.0),\n",
       "  ('Doc2', 'be', 'be', 0.0),\n",
       "  ('Doc2', 'held', 'held', 0.0),\n",
       "  ('Doc2', 'returns', 'returns', 0.0),\n",
       "  ('Doc2', 'from', 'from', 0.0),\n",
       "  ('Doc2', 'recess', 'recess', 0.0),\n",
       "  ('Doc2', 'next', 'next', 0.0),\n",
       "  ('Doc2', 'week', 'week', 0.0),\n",
       "  ('Doc2', 'sweeping', 'sweeping', 0.017719553043034027),\n",
       "  ('Doc2', 'tax', 'tax', 0.017719553043034027),\n",
       "  ('Doc2', 'law', 'law', 0.017719553043034027),\n",
       "  ('Doc2', 'republicans', 'republicans', 0.017719553043034027),\n",
       "  ('Doc2', 'enacted', 'enacted', 0.017719553043034027),\n",
       "  ('Doc2', 'late', 'late', 0.017719553043034027),\n",
       "  ('Doc2', '2017', '2017', 0.017719553043034027),\n",
       "  ('Doc2', 'is', 'is', 0.017719553043034027),\n",
       "  ('Doc2', 'definitely', 'definitely', 0.017719553043034027),\n",
       "  ('Doc2', 'not', 'not', 0.03543910608606805),\n",
       "  ('Doc2', 'paying', 'paying', 0.017719553043034027),\n",
       "  ('Doc2', 'for', 'for', 0.006539759808196199),\n",
       "  ('Doc2', 'itself', 'itself', 0.017719553043034027),\n",
       "  ('Doc2', 'significantly', 'significantly', 0.017719553043034027),\n",
       "  ('Doc2', 'boosted', 'boosted', 0.017719553043034027),\n",
       "  ('Doc2', 'economy', 'economy', 0.017719553043034027),\n",
       "  ('Doc2', 'or', 'or', 0.017719553043034027),\n",
       "  ('Doc2', 'increased', 'increased', 0.017719553043034027),\n",
       "  ('Doc2', 'wages', 'wages', 0.017719553043034027),\n",
       "  ('Doc2', 'nonpartisan', 'nonpartisan', 0.017719553043034027),\n",
       "  ('Doc2', 'congressional', 'congressional', 0.017719553043034027),\n",
       "  ('Doc2', 'research', 'research', 0.017719553043034027),\n",
       "  ('Doc2', 'service', 'service', 0.017719553043034027),\n",
       "  ('Doc2', 'said', 'said', 0.006539759808196199),\n",
       "  ('Doc2', 'report', 'report', 0.017719553043034027),\n",
       "  ('Doc2', 'but', 'but', 0.017719553043034027),\n",
       "  ('Doc2', 'line', 'line', 0.017719553043034027),\n",
       "  ('Doc2', 'with', 'with', 0.017719553043034027),\n",
       "  ('Doc2', 'what', 'what', 0.017719553043034027),\n",
       "  ('Doc2', 'critics', 'critics', 0.017719553043034027),\n",
       "  ('Doc2', 'cautioned', 'cautioned', 0.017719553043034027),\n",
       "  ('Doc2', 'measure', 'measure', 0.017719553043034027),\n",
       "  ('Doc2', 'triggered', 'triggered', 0.017719553043034027),\n",
       "  ('Doc2', 'wave', 'wave', 0.017719553043034027),\n",
       "  ('Doc2', 'of', 'of', 0.006539759808196199),\n",
       "  ('Doc2', 'corporate', 'corporate', 0.017719553043034027),\n",
       "  ('Doc2', 'stock', 'stock', 0.017719553043034027),\n",
       "  ('Doc2', 'buybacks', 'buybacks', 0.017719553043034027),\n",
       "  ('Doc2', 'benefited', 'benefited', 0.017719553043034027),\n",
       "  ('Doc2', 'investors', 'investors', 0.017719553043034027),\n",
       "  ('Doc2', 'more', 'more', 0.017719553043034027),\n",
       "  ('Doc2', 'than', 'than', 0.017719553043034027),\n",
       "  ('Doc2', 'anybody', 'anybody', 0.017719553043034027),\n",
       "  ('Doc2', 'else', 'else', 0.017719553043034027),\n",
       "  ('Doc2', 'according', 'according', 0.017719553043034027),\n",
       "  ('Doc2', 'new', 'new', 0.017719553043034027),\n",
       "  ('Doc2', 'study', 'study', 0.017719553043034027),\n",
       "  ('Doc2', 'senate', 'senate', 0.0),\n",
       "  ('Doc2', 'majority', 'majority', 0.0),\n",
       "  ('Doc2', 'leader', 'leader', 0.0),\n",
       "  ('Doc2', 'mitch', 'mitch', 0.0),\n",
       "  ('Doc2', 'mcconnell', 'mcconnell', 0.0),\n",
       "  ('Doc2', 'he', 'he', 0.0),\n",
       "  ('Doc2', 'work', 'work', 0.0),\n",
       "  ('Doc2', 'fill', 'fill', 0.0),\n",
       "  ('Doc2', 'any', 'any', 0.0),\n",
       "  ('Doc2', 'supreme', 'supreme', 0.0),\n",
       "  ('Doc2', 'court', 'court', 0.0),\n",
       "  ('Doc2', 'vacancy', 'vacancy', 0.0),\n",
       "  ('Doc2', '2020', '2020', 0.0),\n",
       "  ('Doc2', 'an', 'an', 0.0),\n",
       "  ('Doc2', 'election', 'election', 0.0),\n",
       "  ('Doc2', 'year', 'year', 0.0),\n",
       "  ('Doc2', 'despite', 'despite', 0.0),\n",
       "  ('Doc2', 'his', 'his', 0.0),\n",
       "  ('Doc2', 'efforts', 'efforts', 0.0),\n",
       "  ('Doc2', 'scuttle', 'scuttle', 0.0),\n",
       "  ('Doc2', 'judge', 'judge', 0.0),\n",
       "  ('Doc2', 'merrick', 'merrick', 0.0),\n",
       "  ('Doc2', 'garlands', 'garlands', 0.0),\n",
       "  ('Doc2', 'nomination', 'nomination', 0.0),\n",
       "  ('Doc2', 'bench', 'bench', 0.0),\n",
       "  ('Doc2', 'very', 'very', 0.0),\n",
       "  ('Doc2', 'reason', 'reason', 0.0),\n",
       "  ('Doc2', '2016', '2016', 0.0),\n",
       "  ('Doc2', 'uh', 'uh', 0.0),\n",
       "  ('Doc2', 'wed', 'wed', 0.0),\n",
       "  ('Doc2', 'it', 'it', 0.0),\n",
       "  ('Doc2', 'response', 'response', 0.0),\n",
       "  ('Doc2', 'audience', 'audience', 0.0),\n",
       "  ('Doc2', 'question', 'question', 0.0),\n",
       "  ('Doc2', 'event', 'event', 0.0),\n",
       "  ('Doc2', 'at', 'at', 0.0),\n",
       "  ('Doc2', 'paducah', 'paducah', 0.0),\n",
       "  ('Doc2', 'chamber', 'chamber', 0.0),\n",
       "  ('Doc2', 'commerce', 'commerce', 0.0),\n",
       "  ('Doc2', 'kentucky', 'kentucky', 0.0),\n",
       "  ('Doc2', 'afternoon', 'afternoon', 0.0),\n",
       "  ('Doc2', 'lawmaker', 'lawmaker', 0.0),\n",
       "  ('Doc2', 'issued', 'issued', 0.0),\n",
       "  ('Doc2', 'small', 'small', 0.0),\n",
       "  ('Doc2', 'smile', 'smile', 0.0),\n",
       "  ('Doc2', 'answer', 'answer', 0.0),\n",
       "  ('Doc2', 'as', 'as', 0.0),\n",
       "  ('Doc2', 'guests', 'guests', 0.0),\n",
       "  ('Doc2', 'room', 'room', 0.0),\n",
       "  ('Doc2', 'laughed', 'laughed', 0.0)],\n",
       " [('Doc3', 'a', 'a', 0.0),\n",
       "  ('Doc3', 'second', 'second', 0.0),\n",
       "  ('Doc3', 'attempt', 'attempt', 0.0),\n",
       "  ('Doc3', 'to', 'to', 0.0),\n",
       "  ('Doc3', 'pass', 'pass', 0.0),\n",
       "  ('Doc3', 'bipartisan', 'bipartisan', 0.0),\n",
       "  ('Doc3', 'disaster', 'disaster', 0.0),\n",
       "  ('Doc3', 'aid', 'aid', 0.0),\n",
       "  ('Doc3', 'package', 'package', 0.0),\n",
       "  ('Doc3', 'has', 'has', 0.0),\n",
       "  ('Doc3', 'failed', 'failed', 0.0),\n",
       "  ('Doc3', 'after', 'after', 0.0),\n",
       "  ('Doc3', 'house', 'house', 0.0),\n",
       "  ('Doc3', 'republican', 'republican', 0.0),\n",
       "  ('Doc3', 'voted', 'voted', 0.0),\n",
       "  ('Doc3', 'on', 'on', 0.005005742075409436),\n",
       "  ('Doc3', 'tuesday', 'tuesday', 0.010011484150818872),\n",
       "  ('Doc3', 'against', 'against', 0.0),\n",
       "  ('Doc3', 'bill', 'bill', 0.0),\n",
       "  ('Doc3', 'that', 'that', 0.0),\n",
       "  ('Doc3', 'would', 'would', 0.005005742075409436),\n",
       "  ('Doc3', 'secure', 'secure', 0.0),\n",
       "  ('Doc3', '19', '19', 0.0),\n",
       "  ('Doc3', 'billion', 'billion', 0.0),\n",
       "  ('Doc3', 'in', 'in', 0.0),\n",
       "  ('Doc3', 'emergency', 'emergency', 0.0),\n",
       "  ('Doc3', 'relief', 'relief', 0.0),\n",
       "  ('Doc3', 'rep', 'rep', 0.0),\n",
       "  ('Doc3', 'thomas', 'thomas', 0.0),\n",
       "  ('Doc3', 'massie', 'massie', 0.0),\n",
       "  ('Doc3', 'rky', 'rky', 0.005005742075409436),\n",
       "  ('Doc3', 'objected', 'objected', 0.0),\n",
       "  ('Doc3', 'the', 'the', 0.0),\n",
       "  ('Doc3', 'legislations', 'legislations', 0.0),\n",
       "  ('Doc3', 'passage', 'passage', 0.0),\n",
       "  ('Doc3', 'during', 'during', 0.010011484150818872),\n",
       "  ('Doc3', 'voice', 'voice', 0.0),\n",
       "  ('Doc3', 'vote', 'vote', 0.0),\n",
       "  ('Doc3', 'and', 'and', 0.0),\n",
       "  ('Doc3', 'asked', 'asked', 0.0),\n",
       "  ('Doc3', 'another', 'another', 0.0),\n",
       "  ('Doc3', 'be', 'be', 0.0),\n",
       "  ('Doc3', 'held', 'held', 0.0),\n",
       "  ('Doc3', 'returns', 'returns', 0.0),\n",
       "  ('Doc3', 'from', 'from', 0.0),\n",
       "  ('Doc3', 'recess', 'recess', 0.0),\n",
       "  ('Doc3', 'next', 'next', 0.0),\n",
       "  ('Doc3', 'week', 'week', 0.0),\n",
       "  ('Doc3', 'sweeping', 'sweeping', 0.0),\n",
       "  ('Doc3', 'tax', 'tax', 0.0),\n",
       "  ('Doc3', 'law', 'law', 0.0),\n",
       "  ('Doc3', 'republicans', 'republicans', 0.0),\n",
       "  ('Doc3', 'enacted', 'enacted', 0.0),\n",
       "  ('Doc3', 'late', 'late', 0.0),\n",
       "  ('Doc3', '2017', '2017', 0.0),\n",
       "  ('Doc3', 'is', 'is', 0.0),\n",
       "  ('Doc3', 'definitely', 'definitely', 0.0),\n",
       "  ('Doc3', 'not', 'not', 0.0),\n",
       "  ('Doc3', 'paying', 'paying', 0.0),\n",
       "  ('Doc3', 'for', 'for', 0.005005742075409436),\n",
       "  ('Doc3', 'itself', 'itself', 0.0),\n",
       "  ('Doc3', 'significantly', 'significantly', 0.0),\n",
       "  ('Doc3', 'boosted', 'boosted', 0.0),\n",
       "  ('Doc3', 'economy', 'economy', 0.0),\n",
       "  ('Doc3', 'or', 'or', 0.0),\n",
       "  ('Doc3', 'increased', 'increased', 0.0),\n",
       "  ('Doc3', 'wages', 'wages', 0.0),\n",
       "  ('Doc3', 'nonpartisan', 'nonpartisan', 0.0),\n",
       "  ('Doc3', 'congressional', 'congressional', 0.0),\n",
       "  ('Doc3', 'research', 'research', 0.0),\n",
       "  ('Doc3', 'service', 'service', 0.0),\n",
       "  ('Doc3', 'said', 'said', 0.010011484150818872),\n",
       "  ('Doc3', 'report', 'report', 0.0),\n",
       "  ('Doc3', 'but', 'but', 0.0),\n",
       "  ('Doc3', 'line', 'line', 0.0),\n",
       "  ('Doc3', 'with', 'with', 0.0),\n",
       "  ('Doc3', 'what', 'what', 0.0),\n",
       "  ('Doc3', 'critics', 'critics', 0.0),\n",
       "  ('Doc3', 'cautioned', 'cautioned', 0.0),\n",
       "  ('Doc3', 'measure', 'measure', 0.0),\n",
       "  ('Doc3', 'triggered', 'triggered', 0.0),\n",
       "  ('Doc3', 'wave', 'wave', 0.0),\n",
       "  ('Doc3', 'of', 'of', 0.005005742075409436),\n",
       "  ('Doc3', 'corporate', 'corporate', 0.0),\n",
       "  ('Doc3', 'stock', 'stock', 0.0),\n",
       "  ('Doc3', 'buybacks', 'buybacks', 0.0),\n",
       "  ('Doc3', 'benefited', 'benefited', 0.0),\n",
       "  ('Doc3', 'investors', 'investors', 0.0),\n",
       "  ('Doc3', 'more', 'more', 0.0),\n",
       "  ('Doc3', 'than', 'than', 0.0),\n",
       "  ('Doc3', 'anybody', 'anybody', 0.0),\n",
       "  ('Doc3', 'else', 'else', 0.0),\n",
       "  ('Doc3', 'according', 'according', 0.0),\n",
       "  ('Doc3', 'new', 'new', 0.0),\n",
       "  ('Doc3', 'study', 'study', 0.0),\n",
       "  ('Doc3', 'senate', 'senate', 0.013563114674914934),\n",
       "  ('Doc3', 'majority', 'majority', 0.013563114674914934),\n",
       "  ('Doc3', 'leader', 'leader', 0.013563114674914934),\n",
       "  ('Doc3', 'mitch', 'mitch', 0.013563114674914934),\n",
       "  ('Doc3', 'mcconnell', 'mcconnell', 0.027126229349829868),\n",
       "  ('Doc3', 'he', 'he', 0.013563114674914934),\n",
       "  ('Doc3', 'work', 'work', 0.013563114674914934),\n",
       "  ('Doc3', 'fill', 'fill', 0.027126229349829868),\n",
       "  ('Doc3', 'any', 'any', 0.013563114674914934),\n",
       "  ('Doc3', 'supreme', 'supreme', 0.013563114674914934),\n",
       "  ('Doc3', 'court', 'court', 0.013563114674914934),\n",
       "  ('Doc3', 'vacancy', 'vacancy', 0.013563114674914934),\n",
       "  ('Doc3', '2020', '2020', 0.013563114674914934),\n",
       "  ('Doc3', 'an', 'an', 0.040689344024744806),\n",
       "  ('Doc3', 'election', 'election', 0.013563114674914934),\n",
       "  ('Doc3', 'year', 'year', 0.013563114674914934),\n",
       "  ('Doc3', 'despite', 'despite', 0.013563114674914934),\n",
       "  ('Doc3', 'his', 'his', 0.027126229349829868),\n",
       "  ('Doc3', 'efforts', 'efforts', 0.013563114674914934),\n",
       "  ('Doc3', 'scuttle', 'scuttle', 0.013563114674914934),\n",
       "  ('Doc3', 'judge', 'judge', 0.013563114674914934),\n",
       "  ('Doc3', 'merrick', 'merrick', 0.013563114674914934),\n",
       "  ('Doc3', 'garlands', 'garlands', 0.013563114674914934),\n",
       "  ('Doc3', 'nomination', 'nomination', 0.013563114674914934),\n",
       "  ('Doc3', 'bench', 'bench', 0.013563114674914934),\n",
       "  ('Doc3', 'very', 'very', 0.013563114674914934),\n",
       "  ('Doc3', 'reason', 'reason', 0.013563114674914934),\n",
       "  ('Doc3', '2016', '2016', 0.013563114674914934),\n",
       "  ('Doc3', 'uh', 'uh', 0.013563114674914934),\n",
       "  ('Doc3', 'wed', 'wed', 0.013563114674914934),\n",
       "  ('Doc3', 'it', 'it', 0.013563114674914934),\n",
       "  ('Doc3', 'response', 'response', 0.013563114674914934),\n",
       "  ('Doc3', 'audience', 'audience', 0.013563114674914934),\n",
       "  ('Doc3', 'question', 'question', 0.013563114674914934),\n",
       "  ('Doc3', 'event', 'event', 0.013563114674914934),\n",
       "  ('Doc3', 'at', 'at', 0.013563114674914934),\n",
       "  ('Doc3', 'paducah', 'paducah', 0.013563114674914934),\n",
       "  ('Doc3', 'chamber', 'chamber', 0.013563114674914934),\n",
       "  ('Doc3', 'commerce', 'commerce', 0.013563114674914934),\n",
       "  ('Doc3', 'kentucky', 'kentucky', 0.013563114674914934),\n",
       "  ('Doc3', 'afternoon', 'afternoon', 0.013563114674914934),\n",
       "  ('Doc3', 'lawmaker', 'lawmaker', 0.013563114674914934),\n",
       "  ('Doc3', 'issued', 'issued', 0.013563114674914934),\n",
       "  ('Doc3', 'small', 'small', 0.013563114674914934),\n",
       "  ('Doc3', 'smile', 'smile', 0.013563114674914934),\n",
       "  ('Doc3', 'answer', 'answer', 0.013563114674914934),\n",
       "  ('Doc3', 'as', 'as', 0.013563114674914934),\n",
       "  ('Doc3', 'guests', 'guests', 0.013563114674914934),\n",
       "  ('Doc3', 'room', 'room', 0.013563114674914934),\n",
       "  ('Doc3', 'laughed', 'laughed', 0.013563114674914934)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each set of TF-IDF scores (145 x 1 vector) is then input into a classification algorithm of choice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
