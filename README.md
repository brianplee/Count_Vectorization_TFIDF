# Count_Vectorization_TFIDF
Preparing a set of documents for natural language processing from scratch:
- Document pre-processing to remove special characters, punctuation, and extra spaces
- Tokenizing words by splitting strings
- Word count-vectorization across a set of documents 
- TFIDF-score generation 
