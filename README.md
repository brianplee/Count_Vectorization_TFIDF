# Count_Vectorization_TFIDF
Preparing a set of documents for natural language processing from scratch:
- Document pre-processing to remove special characters, punctuation, and extra spaces
- Tokenizing words by splitting strings
- Count-vectorization of words across a set of documents to generate n-dimensional vectors where n = # of unique words in the corpus
- TFIDF-score generation for each word in the vectors to be input in classification algorithms 
